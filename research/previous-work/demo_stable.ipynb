{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T07:22:45.084718700Z",
     "start_time": "2024-02-27T07:22:45.079642700Z"
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"path\": {\n",
    "        \"dataset_path\":\"C:\\\\Users\\\\Cheon\\\\Dev\\\\deepcheese\\\\lob_data\",\n",
    "        \"model_path\":\"C:\\\\Users\\\\Cheon\\\\Dev\\\\deepcheese\\\\models\"\n",
    "    },\n",
    "\n",
    "    \"dataset\": {\n",
    "        \"ticker\": \"KQ150\",\n",
    "        \"open\": \"090000\",\n",
    "        \"close\": \"154500\",\n",
    "        \"orderbook_level\": 5,\n",
    "        \"time_window\": 50,\n",
    "        \"price_window\" : 10,\n",
    "        \"max_amount\": 5,\n",
    "        \"train_ratio\": 0.5,\n",
    "        \"val_ratio\": 0.2,\n",
    "        \"test_ratio\": 0.3,\n",
    "        \"predict_horizon\": 100,\n",
    "        \"threshold\": 0.0002\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T07:22:45.580965400Z",
     "start_time": "2024-02-27T07:22:45.577230600Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "_config_path = config[\"path\"]\n",
    "_config_dataset = config[\"dataset\"]\n",
    "\n",
    "\n",
    "def time_to_idx(timestring):\n",
    "    hour = int(timestring[:2])\n",
    "    minute = int(timestring[2:4])\n",
    "    second = int(timestring[4:])\n",
    "    return (hour - 9) * 60 * 60 + minute * 60 + second\n",
    "\n",
    "\n",
    "def idx_to_time(idx):\n",
    "    second = idx % 60\n",
    "    minute = int((idx - second) / 60) % 60\n",
    "    hour = (idx - second - minute * 60) // (60 * 60)\n",
    "    return '%02d:%02d:%02d' % (hour + 9, minute, second)\n",
    "\n",
    "\n",
    "def get_file_path(data_type, ticker, date=\"*\"):\n",
    "    match data_type:\n",
    "        case \"tradeprint\":\n",
    "            filename1 = \"H0IFCNT0\"\n",
    "        case \"orderbook\":\n",
    "            filename1 = \"H0IFASP0\"\n",
    "\n",
    "    match ticker:\n",
    "        case \"KS200\":\n",
    "            filename2 = \"101T*\"\n",
    "        case \"KQ150\":\n",
    "            filename2 = \"106T*\"\n",
    "\n",
    "    if date == \"*\":\n",
    "        return os.path.join(_config_path[\"dataset_path\"], f\"{filename1}_{filename2}_{date}.csv\")\n",
    "    else:\n",
    "        return glob.glob(os.path.join(_config_path[\"dataset_path\"], f\"{filename1}_{filename2}_{date}.csv\"))[0]\n",
    "\n",
    "\n",
    "def get_file_list(data_type, ticker):\n",
    "    return glob.glob(get_file_path(data_type, ticker))\n",
    "\n",
    "\n",
    "def get_days_list(ticker):\n",
    "    def extract_days(file_list):\n",
    "        day_list = []\n",
    "        for filename in file_list:\n",
    "            day_list.append(filename.split('.')[0].split('_')[-1])\n",
    "        return day_list\n",
    "\n",
    "    # return days which has coupled tradeprint and orderbook data\n",
    "    day_list_1 = extract_days(get_file_list(\"tradeprint\", ticker))\n",
    "    day_list_2 = extract_days(get_file_list(\"orderbook\", ticker))\n",
    "    intersect_day = list(set(day_list_1) & set(day_list_2))\n",
    "    intersect_day.sort()\n",
    "    return intersect_day\n",
    "\n",
    "\n",
    "def load_orderbook_data(filename):\n",
    "    mul_fac = 10\n",
    "    orderbook_level = 5\n",
    "\n",
    "    time_data = []\n",
    "    orderbook_data = []\n",
    "\n",
    "    with open(filename, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for idx, row in enumerate(reader):\n",
    "            temp_timestamp = row[1]\n",
    "            update = len(time_data) == 0 or temp_timestamp != time_data[-1]\n",
    "            if (not row[2] == '0.00') and update:\n",
    "                # Process orderbook data in single timestamp\n",
    "                temp_orderbook_data = []\n",
    "                for i in range(1, orderbook_level + 1):\n",
    "                    single_level = [\n",
    "                        ###################################################\n",
    "                        int(float(row[1+i])*mul_fac), # price  | ask side\n",
    "                        int(row[21+i]),               # volume |\n",
    "                        ###################################################\n",
    "                        int(float(row[6+i])*mul_fac), # price  | bid side\n",
    "                        int(row[26+i])                # volume |\n",
    "                        ###################################################\n",
    "                    ]\n",
    "                    temp_orderbook_data.append(single_level)\n",
    "                time_data.append(temp_timestamp)\n",
    "                orderbook_data.append(temp_orderbook_data)\n",
    "    return time_data, np.array(orderbook_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T07:22:46.310155300Z",
     "start_time": "2024-02-27T07:22:46.275468300Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MovingWindow:\n",
    "    def __init__(self, day):\n",
    "        self.day = day\n",
    "        self.ticker = _config_dataset[\"ticker\"]\n",
    "        self.level = _config_dataset[\"orderbook_level\"]\n",
    "        self.T = _config_dataset[\"time_window\"]\n",
    "        self.W = _config_dataset[\"price_window\"]\n",
    "        self.max_quantity = _config_dataset[\"max_amount\"]\n",
    "        self.predict_horizon = _config_dataset[\"predict_horizon\"]\n",
    "        self.threshold = _config_dataset[\"threshold\"]\n",
    "\n",
    "\n",
    "        file_path = get_file_path(\"orderbook\", self.ticker, day)\n",
    "        self.orderbook_time, self.orderbook_data = load_orderbook_data(file_path)\n",
    "        self.movingwindow = []\n",
    "        self.ticks = []\n",
    "\n",
    "        self.start_idx = max(self.T - 1, self.predict_horizon)\n",
    "        self.end_idx = len(self.orderbook_data) - self.predict_horizon - 1\n",
    "\n",
    "        self.midprice = []\n",
    "        self.ask1 = []\n",
    "        self.bid1 = []\n",
    "        self.midprice_idx = []\n",
    "        self.__fill_movingwindow__(self.orderbook_data)\n",
    "\n",
    "        self.label = []\n",
    "        self.__fill_label__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.end_idx - self.start_idx + 1\n",
    "    \n",
    "    def __moving_window_shot__(self, lob, ticks, data_min, data_max):\n",
    "        lob_shot = lob.copy()\n",
    "        lob_shot[:, 1:4:2] = np.minimum(lob_shot[:, 1:4:2], self.max_quantity)\n",
    "        lob_shot[:,3] = -lob_shot[:,3]\n",
    "\n",
    "        for i in range(1, lob_shot.shape[0]):\n",
    "            lob_shot[i, 1:4:2] = lob_shot[i-1, 1:4:2] + lob_shot[i, 1:4:2]\n",
    "\n",
    "        ask_side = lob_shot[:, 0:2]\n",
    "        bid_side = lob_shot[::-1, 2:4]\n",
    "\n",
    "        midprice = (ask_side[0, 0] + bid_side[0, 0]) / 2\n",
    "        ask1 = ask_side[0, 0]\n",
    "        bid1 = bid_side[0, 0]\n",
    "        midprice_idx = round(midprice - (data_min - self.W))\n",
    "\n",
    "        moving_window_shot = np.zeros(ticks.shape)\n",
    "\n",
    "        # bid side\n",
    "        for i in range(5):\n",
    "            if i == 0:\n",
    "                start_idx = 0\n",
    "            else:\n",
    "                start_idx = bid_side[i-1, 0] - (data_min - self.W) + 1\n",
    "            end_idx = bid_side[i, 0] - (data_min - self.W)\n",
    "            moving_window_shot[start_idx:end_idx+1] = bid_side[i, 1]\n",
    "\n",
    "        # ask side\n",
    "        for i in range(5):\n",
    "            if i == 4:\n",
    "                end_idx = len(moving_window_shot)\n",
    "            else:\n",
    "                end_idx = ask_side[i + 1, 0] - (data_min - self.W) + 1\n",
    "            start_idx = ask_side[i, 0] - (data_min - self.W)\n",
    "            moving_window_shot[start_idx:end_idx+1] = ask_side[i, 1]\n",
    "\n",
    "        return moving_window_shot, midprice, ask1, bid1, midprice_idx\n",
    "\n",
    "\n",
    "    def __fill_movingwindow__(self, data):\n",
    "        processed_data = []\n",
    "        midprice = []\n",
    "        ask1 = []\n",
    "        bid1 = []\n",
    "        midprice_idx = []\n",
    "\n",
    "        data_min = np.min(data[:,:,0:3:2].flatten())\n",
    "        data_max = np.max(data[:,:,0:3:2].flatten())\n",
    "        ticks = np.arange(data_min - self.W, data_max + self.W + 1, 1)\n",
    "\n",
    "        for i in range(data.shape[0]):\n",
    "            lob_shot, midprice_, ask_1, bid_1, midprice_idx_ = self.__moving_window_shot__(data[i], ticks, data_min, data_max)\n",
    "            processed_data.append(lob_shot)\n",
    "            midprice.append(midprice_)\n",
    "            ask1.append(ask_1)\n",
    "            bid1.append(bid_1)\n",
    "            midprice_idx.append(midprice_idx_)\n",
    "\n",
    "        self.movingwindow = np.array(processed_data) / (self.max_quantity * 5)\n",
    "        self.ticks = ticks\n",
    "        self.midprice = np.array(midprice)\n",
    "        self.ask1 = np.array(ask1)\n",
    "        self.bid1 = np.array(bid1)\n",
    "        self.midprice_idx = np.array(midprice_idx)\n",
    "        \n",
    "\n",
    "    def __fill_label__(self):\n",
    "        label = np.zeros(self.end_idx - self.start_idx + 1)\n",
    "\n",
    "        for index in range(self.end_idx - self.start_idx + 1):\n",
    "            index_re = index + self.start_idx\n",
    "            m_prev = np.mean(self.midprice[index_re - self.predict_horizon:index_re])\n",
    "            m_next = np.mean(self.midprice[index_re + 1:index_re + self.predict_horizon + 1])\n",
    "            l = (m_next - m_prev) / m_prev\n",
    "\n",
    "            if l > self.threshold:\n",
    "                tmp_label = 2\n",
    "            elif l < -self.threshold:\n",
    "                tmp_label = 0\n",
    "            else:\n",
    "                tmp_label = 1\n",
    "            label[index] = tmp_label\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index_re = index + self.start_idx\n",
    "        shot = self.movingwindow[index_re-self.T+1:index_re+1,\n",
    "                                 self.midprice_idx[index_re]-self.W:self.midprice_idx[index_re]+self.W].T\n",
    "        return np.expand_dims(shot, axis=0), self.label[index]\n",
    "    \n",
    "    \n",
    "    def get_price(self):\n",
    "        return self.midprice[self.start_idx:self.end_idx + 1], \\\n",
    "               self.ask1[self.start_idx:self.end_idx + 1], \\\n",
    "               self.bid1[self.start_idx:self.end_idx + 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T07:22:49.781778600Z",
     "start_time": "2024-02-27T07:22:49.771997800Z"
    }
   },
   "outputs": [],
   "source": [
    "from ray.util.multiprocessing import Pool\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class MarketEnv(gym.Env):\n",
    "    def __init__(self, days) -> None:\n",
    "        self.days = days\n",
    "        with Pool() as pool:\n",
    "            self.day_datasets = pool.map(MovingWindow, self.days)\n",
    "        \n",
    "        self.observation_space = spaces.Box(low=-1, high=1,\n",
    "                                            shape=(1, 20, 50),\n",
    "                                            dtype=np.float64)\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        self.day_idx = np.random.randint(len(self.days))\n",
    "        self.day = self.days[self.day_idx]\n",
    "        self.day_dataset = self.day_datasets[self.day_idx]\n",
    "        self.day_len = len(self.day_dataset)\n",
    "        self.max_position = 1\n",
    "        self.fee = 0.003 / 100\n",
    "\n",
    "        self.t = 0\n",
    "        self.t_max = self.day_len - 1\n",
    "\n",
    "        self.midprice, self.ask1, self.bid1 = self.day_dataset.get_price()\n",
    "        self.cumulative_reward = np.zeros(self.day_len)\n",
    "        self.position = np.zeros(self.day_len)\n",
    "        self.cash = np.zeros(self.day_len)\n",
    "        self.balance = np.zeros(self.day_len)\n",
    "        \n",
    "        self.long_inventory = []\n",
    "        self.short_inventory = []\n",
    "\n",
    "        return self.day_dataset[self.t][0], {}#, np.array([0.0])\n",
    "\n",
    "    def step(self, action):\n",
    "        self.t += 1\n",
    "        \n",
    "        # action: 0 - hold, 1 - buy, 2 - sell\n",
    "        if action == 1 and self.position[self.t - 1] < self.max_position:\n",
    "            position_change = 1\n",
    "        elif action == 2 and self.position[self.t - 1] > -self.max_position:\n",
    "            position_change = -1\n",
    "        else:\n",
    "            position_change = 0\n",
    "        \n",
    "        # trading price\n",
    "        if position_change == 1:\n",
    "            trade_price = self.ask1[self.t]\n",
    "        elif position_change == -1:\n",
    "            trade_price = self.bid1[self.t]\n",
    "        else:\n",
    "            trade_price = self.midprice[self.t]\n",
    "        \n",
    "        self.position[self.t] = self.position[self.t - 1] + position_change\n",
    "        self.cash[self.t] = self.cash[self.t - 1] - trade_price * position_change - abs(position_change) * self.fee * trade_price\n",
    "        self.balance[self.t] = self.cash[self.t] + self.position[self.t] * self.midprice[self.t]\n",
    "\n",
    "        reward = self.balance[self.t] - self.balance[self.t - 1]\n",
    "            \n",
    "        self.cumulative_reward[self.t] = self.cumulative_reward[self.t - 1] + reward\n",
    "        done = self.t == self.t_max\n",
    "\n",
    "        if done:\n",
    "            state = None\n",
    "            info = {\n",
    "                \"position\": self.position,\n",
    "                \"balance\": self.balance,\n",
    "                \"cumulative_reward\": self.cumulative_reward,\n",
    "                \"midprice\": self.midprice\n",
    "            }\n",
    "        else:\n",
    "            state = self.day_dataset[self.t][0]\n",
    "            info = {\n",
    "                \"position\": self.position[self.t],\n",
    "                \"balance\": self.balance[self.t],\n",
    "                \"cumulative_reward\": self.cumulative_reward[self.t],\n",
    "                \"midprice\": self.midprice[self.t]\n",
    "            }\n",
    "        \n",
    "        state_pos = np.array([self.position[self.t]/self.max_position])\n",
    "        truncated = False\n",
    "        return state, reward, done, truncated, info\n",
    "    \n",
    "    def evaluate_balance(self):\n",
    "        self.position_history[self.t] = self.position\n",
    "        self.balance_history[self.t] = self.balance\n",
    "        self.cumulative_reward_history[self.t] = self.cumulative_reward\n",
    "        return self.balance_history[-1]\n",
    "    \n",
    "    def close(self):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T07:22:50.803409400Z",
     "start_time": "2024-02-27T07:22:50.794779800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% 38 days for train (2023-04-03 ~ 2023-05-26)\n",
      "% 15 days for validation (2023-05-30 ~ 2023-06-21)\n",
      "% 23 days for test (2023-06-22 ~ 2023-07-24)\n"
     ]
    }
   ],
   "source": [
    "days = get_days_list(_config_dataset[\"ticker\"])\n",
    "\n",
    "val_length = max(round(len(days) * _config_dataset[\"val_ratio\"]), 1)\n",
    "test_length = max(round(len(days) * _config_dataset[\"test_ratio\"]), 1)\n",
    "train_length = len(days) - (val_length + test_length)\n",
    "\n",
    "train_days = days[:train_length]\n",
    "val_days = days[train_length:train_length + val_length]\n",
    "test_days = days[train_length + val_length:]\n",
    "\n",
    "print(f\"% {len(train_days)} days for train ({train_days[0]} ~ {train_days[-1]})\")\n",
    "print(f\"% {len(val_days)} days for validation ({val_days[0]} ~ {val_days[-1]})\")\n",
    "print(f\"% {len(test_days)} days for test ({test_days[0]} ~ {test_days[-1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T07:23:17.702521300Z",
     "start_time": "2024-02-27T07:22:52.410011700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cheon\\Dev\\deepcheese\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\env_checker.py:30: UserWarning: It seems that your observation  is an image but its `dtype` is (float64) whereas it has to be `np.uint8`. If your observation is not an image, we recommend you to flatten the observation to have only a 1D vector\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cheon\\Dev\\deepcheese\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\env_checker.py:38: UserWarning: It seems that your observation space  is an image but the upper and lower bounds are not in [0, 255]. Because the CNN policy normalize automatically the observation you may encounter issue if the values are not in that range.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cheon\\Dev\\deepcheese\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\env_checker.py:51: UserWarning: The minimal resolution for an image is 36x36 for the default `CnnPolicy`. You might need to use a custom features extractor cf. https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "train_env = MarketEnv(train_days)\n",
    "val_env = MarketEnv(val_days)\n",
    "test_env = MarketEnv(test_days)\n",
    "\n",
    "check_env(train_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T07:23:32.070170100Z",
     "start_time": "2024-02-27T07:23:32.012867Z"
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "You should use NatureCNN only with images not with Box(-1.0, 1.0, (1, 20, 50), float64)\n(you are probably using `CnnPolicy` instead of `MlpPolicy` or `MultiInputPolicy`)\nIf you are using a custom environment,\nplease check it using our env checker:\nhttps://stable-baselines3.readthedocs.io/en/master/common/env_checker.html.\nIf you are using `VecNormalize` or already normalized channel-first images you should pass `normalize_images=False`: \nhttps://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 8\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mstable_baselines3\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcommon\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcallbacks\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m EvalCallback\n\u001B[0;32m      4\u001B[0m eval_callback \u001B[38;5;241m=\u001B[39m EvalCallback(val_env, best_model_save_path\u001B[38;5;241m=\u001B[39m_config_path[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_path\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m      5\u001B[0m                                 log_path\u001B[38;5;241m=\u001B[39m_config_path[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_path\u001B[39m\u001B[38;5;124m\"\u001B[39m], eval_freq\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100000\u001B[39m,\n\u001B[0;32m      6\u001B[0m                                 deterministic\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, render\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, n_eval_episodes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m)\n\u001B[1;32m----> 8\u001B[0m rl_model \u001B[38;5;241m=\u001B[39m \u001B[43mPPO\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mCnnPolicy\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_env\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Dev\\deepcheese\\.venv\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:171\u001B[0m, in \u001B[0;36mPPO.__init__\u001B[1;34m(self, policy, env, learning_rate, n_steps, batch_size, n_epochs, gamma, gae_lambda, clip_range, clip_range_vf, normalize_advantage, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, rollout_buffer_class, rollout_buffer_kwargs, target_kl, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001B[0m\n\u001B[0;32m    168\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_kl \u001B[38;5;241m=\u001B[39m target_kl\n\u001B[0;32m    170\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _init_setup_model:\n\u001B[1;32m--> 171\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_setup_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Dev\\deepcheese\\.venv\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:174\u001B[0m, in \u001B[0;36mPPO._setup_model\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    173\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_setup_model\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 174\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_setup_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    176\u001B[0m     \u001B[38;5;66;03m# Initialize schedules for policy/value clipping\u001B[39;00m\n\u001B[0;32m    177\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclip_range \u001B[38;5;241m=\u001B[39m get_schedule_fn(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclip_range)\n",
      "File \u001B[1;32m~\\Dev\\deepcheese\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:133\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm._setup_model\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    121\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrollout_buffer_class \u001B[38;5;241m=\u001B[39m RolloutBuffer\n\u001B[0;32m    123\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrollout_buffer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrollout_buffer_class(\n\u001B[0;32m    124\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_steps,\n\u001B[0;32m    125\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservation_space,  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    131\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrollout_buffer_kwargs,\n\u001B[0;32m    132\u001B[0m )\n\u001B[1;32m--> 133\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy_class\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[assignment]\u001B[39;49;00m\n\u001B[0;32m    134\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobservation_space\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maction_space\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlr_schedule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_sde\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muse_sde\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy_kwargs\u001B[49m\n\u001B[0;32m    135\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n",
      "File \u001B[1;32m~\\Dev\\deepcheese\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:816\u001B[0m, in \u001B[0;36mActorCriticCnnPolicy.__init__\u001B[1;34m(self, observation_space, action_space, lr_schedule, net_arch, activation_fn, ortho_init, use_sde, log_std_init, full_std, use_expln, squash_output, features_extractor_class, features_extractor_kwargs, share_features_extractor, normalize_images, optimizer_class, optimizer_kwargs)\u001B[0m\n\u001B[0;32m    796\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[0;32m    797\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    798\u001B[0m     observation_space: spaces\u001B[38;5;241m.\u001B[39mSpace,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    814\u001B[0m     optimizer_kwargs: Optional[Dict[\u001B[38;5;28mstr\u001B[39m, Any]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    815\u001B[0m ):\n\u001B[1;32m--> 816\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    817\u001B[0m \u001B[43m        \u001B[49m\u001B[43mobservation_space\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    818\u001B[0m \u001B[43m        \u001B[49m\u001B[43maction_space\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    819\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlr_schedule\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    820\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnet_arch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    821\u001B[0m \u001B[43m        \u001B[49m\u001B[43mactivation_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    822\u001B[0m \u001B[43m        \u001B[49m\u001B[43mortho_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    823\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_sde\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    824\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlog_std_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    825\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfull_std\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    826\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_expln\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    827\u001B[0m \u001B[43m        \u001B[49m\u001B[43msquash_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    828\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfeatures_extractor_class\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    829\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfeatures_extractor_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    830\u001B[0m \u001B[43m        \u001B[49m\u001B[43mshare_features_extractor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    831\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnormalize_images\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    832\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptimizer_class\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    833\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptimizer_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    834\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Dev\\deepcheese\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:505\u001B[0m, in \u001B[0;36mActorCriticPolicy.__init__\u001B[1;34m(self, observation_space, action_space, lr_schedule, net_arch, activation_fn, ortho_init, use_sde, log_std_init, full_std, use_expln, squash_output, features_extractor_class, features_extractor_kwargs, share_features_extractor, normalize_images, optimizer_class, optimizer_kwargs)\u001B[0m\n\u001B[0;32m    502\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mortho_init \u001B[38;5;241m=\u001B[39m ortho_init\n\u001B[0;32m    504\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshare_features_extractor \u001B[38;5;241m=\u001B[39m share_features_extractor\n\u001B[1;32m--> 505\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures_extractor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmake_features_extractor\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    506\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures_dim \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures_extractor\u001B[38;5;241m.\u001B[39mfeatures_dim\n\u001B[0;32m    507\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshare_features_extractor:\n",
      "File \u001B[1;32m~\\Dev\\deepcheese\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:120\u001B[0m, in \u001B[0;36mBaseModel.make_features_extractor\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmake_features_extractor\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BaseFeaturesExtractor:\n\u001B[0;32m    119\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Helper method to create a features extractor.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 120\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeatures_extractor_class\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobservation_space\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeatures_extractor_kwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Dev\\deepcheese\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\torch_layers.py:77\u001B[0m, in \u001B[0;36mNatureCNN.__init__\u001B[1;34m(self, observation_space, features_dim, normalized_image)\u001B[0m\n\u001B[0;32m     74\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(observation_space, features_dim)\n\u001B[0;32m     75\u001B[0m \u001B[38;5;66;03m# We assume CxHxW images (channels first)\u001B[39;00m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;66;03m# Re-ordering will be done by pre-preprocessing or wrapper\u001B[39;00m\n\u001B[1;32m---> 77\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m is_image_space(observation_space, check_channels\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, normalized_image\u001B[38;5;241m=\u001B[39mnormalized_image), (\n\u001B[0;32m     78\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou should use NatureCNN \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     79\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124monly with images not with \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mobservation_space\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     80\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(you are probably using `CnnPolicy` instead of `MlpPolicy` or `MultiInputPolicy`)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     81\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIf you are using a custom environment,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     82\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mplease check it using our env checker:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     83\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://stable-baselines3.readthedocs.io/en/master/common/env_checker.html.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     84\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIf you are using `VecNormalize` or already normalized channel-first images \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     85\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou should pass `normalize_images=False`: \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     86\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     87\u001B[0m )\n\u001B[0;32m     88\u001B[0m n_input_channels \u001B[38;5;241m=\u001B[39m observation_space\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     89\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcnn \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mSequential(\n\u001B[0;32m     90\u001B[0m     nn\u001B[38;5;241m.\u001B[39mConv2d(n_input_channels, \u001B[38;5;241m32\u001B[39m, kernel_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m, stride\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m),\n\u001B[0;32m     91\u001B[0m     nn\u001B[38;5;241m.\u001B[39mReLU(),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     96\u001B[0m     nn\u001B[38;5;241m.\u001B[39mFlatten(),\n\u001B[0;32m     97\u001B[0m )\n",
      "\u001B[1;31mAssertionError\u001B[0m: You should use NatureCNN only with images not with Box(-1.0, 1.0, (1, 20, 50), float64)\n(you are probably using `CnnPolicy` instead of `MlpPolicy` or `MultiInputPolicy`)\nIf you are using a custom environment,\nplease check it using our env checker:\nhttps://stable-baselines3.readthedocs.io/en/master/common/env_checker.html.\nIf you are using `VecNormalize` or already normalized channel-first images you should pass `normalize_images=False`: \nhttps://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "eval_callback = EvalCallback(val_env, best_model_save_path=_config_path[\"model_path\"],\n",
    "                                log_path=_config_path[\"model_path\"], eval_freq=100000,\n",
    "                                deterministic=True, render=True, n_eval_episodes=5)\n",
    "\n",
    "rl_model = PPO(\"CnnPolicy\", train_env, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T10:45:08.691160Z",
     "start_time": "2024-02-27T07:23:47.036563700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=100000, episode_reward=-75.10 +/- 278.18\n",
      "Episode length: 22584.20 +/- 361.81\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=-45.67 +/- 291.77\n",
      "Episode length: 22117.00 +/- 781.63\n",
      "New best mean reward!\n",
      "Eval num_timesteps=300000, episode_reward=-116.30 +/- 285.58\n",
      "Episode length: 22349.00 +/- 447.59\n",
      "Eval num_timesteps=400000, episode_reward=95.70 +/- 77.77\n",
      "Episode length: 21640.20 +/- 571.87\n",
      "New best mean reward!\n",
      "Eval num_timesteps=500000, episode_reward=-213.91 +/- 330.37\n",
      "Episode length: 22434.60 +/- 771.09\n",
      "Eval num_timesteps=600000, episode_reward=125.30 +/- 59.02\n",
      "Episode length: 21732.00 +/- 530.92\n",
      "New best mean reward!\n",
      "Eval num_timesteps=700000, episode_reward=-4.30 +/- 147.01\n",
      "Episode length: 21262.20 +/- 1408.51\n",
      "Eval num_timesteps=800000, episode_reward=-95.30 +/- 82.80\n",
      "Episode length: 21432.80 +/- 385.60\n",
      "Eval num_timesteps=900000, episode_reward=-118.10 +/- 290.39\n",
      "Episode length: 22519.60 +/- 379.19\n",
      "Eval num_timesteps=1000000, episode_reward=-45.01 +/- 295.89\n",
      "Episode length: 21712.80 +/- 893.95\n",
      "Eval num_timesteps=1100000, episode_reward=-454.02 +/- 406.02\n",
      "Episode length: 21457.00 +/- 1589.29\n",
      "Eval num_timesteps=1200000, episode_reward=-123.30 +/- 263.16\n",
      "Episode length: 22647.60 +/- 766.46\n",
      "Eval num_timesteps=1300000, episode_reward=55.30 +/- 83.40\n",
      "Episode length: 21501.60 +/- 1493.57\n",
      "Eval num_timesteps=1400000, episode_reward=-198.39 +/- 322.08\n",
      "Episode length: 22040.00 +/- 841.21\n",
      "Eval num_timesteps=1500000, episode_reward=120.90 +/- 75.44\n",
      "Episode length: 22048.00 +/- 561.75\n",
      "Eval num_timesteps=1600000, episode_reward=58.50 +/- 99.96\n",
      "Episode length: 21819.40 +/- 723.04\n",
      "Eval num_timesteps=1700000, episode_reward=142.80 +/- 43.73\n",
      "Episode length: 21949.20 +/- 534.20\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1800000, episode_reward=-129.20 +/- 244.38\n",
      "Episode length: 21764.20 +/- 938.70\n",
      "Eval num_timesteps=1900000, episode_reward=16.40 +/- 68.80\n",
      "Episode length: 21572.40 +/- 576.51\n",
      "Eval num_timesteps=2000000, episode_reward=45.90 +/- 88.93\n",
      "Episode length: 21871.20 +/- 771.75\n",
      "Eval num_timesteps=2100000, episode_reward=50.90 +/- 68.84\n",
      "Episode length: 22017.60 +/- 585.74\n",
      "Eval num_timesteps=2200000, episode_reward=5.50 +/- 162.94\n",
      "Episode length: 22506.60 +/- 383.19\n",
      "Eval num_timesteps=2300000, episode_reward=-48.30 +/- 294.94\n",
      "Episode length: 21759.80 +/- 924.51\n",
      "Eval num_timesteps=2400000, episode_reward=35.50 +/- 165.13\n",
      "Episode length: 22138.00 +/- 619.07\n",
      "Eval num_timesteps=2500000, episode_reward=-20.70 +/- 69.04\n",
      "Episode length: 22106.60 +/- 937.91\n",
      "Eval num_timesteps=2600000, episode_reward=62.40 +/- 105.91\n",
      "Episode length: 20998.80 +/- 1313.19\n",
      "Eval num_timesteps=2700000, episode_reward=114.50 +/- 46.61\n",
      "Episode length: 22029.00 +/- 419.69\n",
      "Eval num_timesteps=2800000, episode_reward=61.90 +/- 81.29\n",
      "Episode length: 21884.40 +/- 675.99\n",
      "Eval num_timesteps=2900000, episode_reward=48.28 +/- 94.59\n",
      "Episode length: 21463.20 +/- 1632.97\n",
      "Eval num_timesteps=3000000, episode_reward=-38.10 +/- 291.41\n",
      "Episode length: 22281.80 +/- 653.12\n",
      "Eval num_timesteps=3100000, episode_reward=-41.00 +/- 182.30\n",
      "Episode length: 22041.20 +/- 420.57\n",
      "Eval num_timesteps=3200000, episode_reward=57.10 +/- 182.32\n",
      "Episode length: 21499.20 +/- 678.26\n",
      "Eval num_timesteps=3300000, episode_reward=-76.44 +/- 292.15\n",
      "Episode length: 21715.20 +/- 892.66\n",
      "Eval num_timesteps=3400000, episode_reward=114.60 +/- 89.56\n",
      "Episode length: 21865.40 +/- 646.92\n",
      "Eval num_timesteps=3500000, episode_reward=54.42 +/- 101.94\n",
      "Episode length: 22243.40 +/- 739.13\n",
      "Eval num_timesteps=3600000, episode_reward=51.90 +/- 162.94\n",
      "Episode length: 21861.60 +/- 457.57\n",
      "Eval num_timesteps=3700000, episode_reward=-41.40 +/- 31.52\n",
      "Episode length: 21841.40 +/- 1668.58\n",
      "Eval num_timesteps=3800000, episode_reward=-13.60 +/- 300.94\n",
      "Episode length: 22466.60 +/- 440.80\n",
      "Eval num_timesteps=3900000, episode_reward=-55.10 +/- 112.83\n",
      "Episode length: 21784.60 +/- 598.54\n",
      "Eval num_timesteps=4000000, episode_reward=18.28 +/- 124.40\n",
      "Episode length: 21112.20 +/- 1522.47\n",
      "Eval num_timesteps=4100000, episode_reward=100.80 +/- 105.17\n",
      "Episode length: 22132.00 +/- 715.79\n",
      "Eval num_timesteps=4200000, episode_reward=65.30 +/- 109.86\n",
      "Episode length: 22224.00 +/- 725.81\n",
      "Eval num_timesteps=4300000, episode_reward=30.64 +/- 105.69\n",
      "Episode length: 21688.20 +/- 1621.05\n",
      "Eval num_timesteps=4400000, episode_reward=-284.51 +/- 270.23\n",
      "Episode length: 21822.40 +/- 1863.35\n",
      "Eval num_timesteps=4500000, episode_reward=-335.31 +/- 235.96\n",
      "Episode length: 23001.60 +/- 400.25\n",
      "Eval num_timesteps=4600000, episode_reward=-123.56 +/- 286.41\n",
      "Episode length: 22334.40 +/- 455.77\n",
      "Eval num_timesteps=4700000, episode_reward=29.54 +/- 81.67\n",
      "Episode length: 21882.00 +/- 780.64\n",
      "Eval num_timesteps=4800000, episode_reward=117.90 +/- 39.48\n",
      "Episode length: 22064.40 +/- 469.09\n",
      "Eval num_timesteps=4900000, episode_reward=-16.96 +/- 146.98\n",
      "Episode length: 21586.00 +/- 1530.00\n",
      "Eval num_timesteps=5000000, episode_reward=-9.40 +/- 143.35\n",
      "Episode length: 21247.60 +/- 1399.84\n",
      "Eval num_timesteps=5100000, episode_reward=151.19 +/- 10.27\n",
      "Episode length: 22369.60 +/- 242.04\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5200000, episode_reward=-55.90 +/- 296.14\n",
      "Episode length: 22276.20 +/- 847.54\n",
      "Eval num_timesteps=5300000, episode_reward=29.00 +/- 42.92\n",
      "Episode length: 21751.80 +/- 582.19\n",
      "Eval num_timesteps=5400000, episode_reward=94.90 +/- 106.33\n",
      "Episode length: 22070.60 +/- 838.27\n",
      "Eval num_timesteps=5500000, episode_reward=21.00 +/- 170.24\n",
      "Episode length: 22493.40 +/- 351.32\n",
      "Eval num_timesteps=5600000, episode_reward=42.50 +/- 83.95\n",
      "Episode length: 22361.40 +/- 440.21\n",
      "Eval num_timesteps=5700000, episode_reward=63.30 +/- 97.99\n",
      "Episode length: 21943.80 +/- 595.91\n",
      "Eval num_timesteps=5800000, episode_reward=116.18 +/- 19.65\n",
      "Episode length: 20057.40 +/- 1855.02\n",
      "Eval num_timesteps=5900000, episode_reward=-38.00 +/- 198.83\n",
      "Episode length: 21748.00 +/- 662.82\n",
      "Eval num_timesteps=6000000, episode_reward=3.80 +/- 80.42\n",
      "Episode length: 21642.40 +/- 850.05\n",
      "Eval num_timesteps=6100000, episode_reward=62.90 +/- 87.23\n",
      "Episode length: 21548.00 +/- 643.90\n",
      "Eval num_timesteps=6200000, episode_reward=105.71 +/- 94.56\n",
      "Episode length: 21289.20 +/- 1476.35\n",
      "Eval num_timesteps=6300000, episode_reward=-240.80 +/- 307.02\n",
      "Episode length: 22076.80 +/- 1034.03\n",
      "Eval num_timesteps=6400000, episode_reward=-16.20 +/- 136.30\n",
      "Episode length: 22175.40 +/- 486.17\n",
      "Eval num_timesteps=6500000, episode_reward=22.50 +/- 321.02\n",
      "Episode length: 21962.80 +/- 886.96\n",
      "Eval num_timesteps=6600000, episode_reward=-181.91 +/- 363.53\n",
      "Episode length: 22066.40 +/- 1027.83\n",
      "Eval num_timesteps=6700000, episode_reward=-39.16 +/- 125.52\n",
      "Episode length: 21732.00 +/- 1594.59\n",
      "Eval num_timesteps=6800000, episode_reward=-189.68 +/- 250.67\n",
      "Episode length: 21376.20 +/- 1595.46\n",
      "Eval num_timesteps=6900000, episode_reward=60.90 +/- 86.79\n",
      "Episode length: 21763.20 +/- 595.91\n",
      "Eval num_timesteps=7000000, episode_reward=117.10 +/- 89.98\n",
      "Episode length: 21764.40 +/- 570.07\n",
      "Eval num_timesteps=7100000, episode_reward=-52.60 +/- 130.74\n",
      "Episode length: 20485.80 +/- 1652.93\n",
      "Eval num_timesteps=7200000, episode_reward=48.80 +/- 116.50\n",
      "Episode length: 22589.60 +/- 491.98\n",
      "Eval num_timesteps=7300000, episode_reward=-27.20 +/- 309.79\n",
      "Episode length: 21134.00 +/- 1542.31\n",
      "Eval num_timesteps=7400000, episode_reward=-110.80 +/- 263.67\n",
      "Episode length: 21715.40 +/- 989.38\n",
      "Eval num_timesteps=7500000, episode_reward=82.30 +/- 115.01\n",
      "Episode length: 22325.00 +/- 729.98\n",
      "Eval num_timesteps=7600000, episode_reward=0.83 +/- 159.85\n",
      "Episode length: 22117.40 +/- 460.61\n",
      "Eval num_timesteps=7700000, episode_reward=123.60 +/- 32.55\n",
      "Episode length: 22251.60 +/- 254.32\n",
      "Eval num_timesteps=7800000, episode_reward=4.60 +/- 160.24\n",
      "Episode length: 22414.80 +/- 394.64\n",
      "Eval num_timesteps=7900000, episode_reward=105.28 +/- 69.86\n",
      "Episode length: 21972.60 +/- 575.85\n",
      "Eval num_timesteps=8000000, episode_reward=15.40 +/- 62.78\n",
      "Episode length: 21195.20 +/- 403.12\n",
      "Eval num_timesteps=8100000, episode_reward=35.40 +/- 85.35\n",
      "Episode length: 21613.60 +/- 1549.76\n",
      "Eval num_timesteps=8200000, episode_reward=116.10 +/- 46.23\n",
      "Episode length: 22051.40 +/- 443.61\n",
      "Eval num_timesteps=8300000, episode_reward=-241.20 +/- 309.85\n",
      "Episode length: 22154.00 +/- 969.15\n",
      "Eval num_timesteps=8400000, episode_reward=157.49 +/- 24.55\n",
      "Episode length: 21980.80 +/- 543.93\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8500000, episode_reward=-115.20 +/- 267.33\n",
      "Episode length: 22812.40 +/- 477.71\n",
      "Eval num_timesteps=8600000, episode_reward=68.80 +/- 95.82\n",
      "Episode length: 21250.80 +/- 1449.16\n",
      "Eval num_timesteps=8700000, episode_reward=58.70 +/- 72.01\n",
      "Episode length: 21067.00 +/- 1329.90\n",
      "Eval num_timesteps=8800000, episode_reward=41.00 +/- 104.30\n",
      "Episode length: 21565.00 +/- 1633.55\n",
      "Eval num_timesteps=8900000, episode_reward=-1.93 +/- 310.64\n",
      "Episode length: 22195.80 +/- 732.50\n",
      "Eval num_timesteps=9000000, episode_reward=76.60 +/- 115.25\n",
      "Episode length: 20587.60 +/- 1754.49\n",
      "Eval num_timesteps=9100000, episode_reward=-497.09 +/- 211.20\n",
      "Episode length: 20721.40 +/- 1787.04\n",
      "Eval num_timesteps=9200000, episode_reward=-20.80 +/- 69.93\n",
      "Episode length: 22053.60 +/- 742.84\n",
      "Eval num_timesteps=9300000, episode_reward=105.90 +/- 63.95\n",
      "Episode length: 22318.80 +/- 231.01\n",
      "Eval num_timesteps=9400000, episode_reward=-90.10 +/- 278.74\n",
      "Episode length: 22348.20 +/- 852.28\n",
      "Eval num_timesteps=9500000, episode_reward=-51.80 +/- 292.43\n",
      "Episode length: 21759.60 +/- 1649.35\n",
      "Eval num_timesteps=9600000, episode_reward=37.40 +/- 86.12\n",
      "Episode length: 22046.80 +/- 708.85\n",
      "Eval num_timesteps=9700000, episode_reward=-100.40 +/- 296.05\n",
      "Episode length: 22334.60 +/- 651.34\n",
      "Eval num_timesteps=9800000, episode_reward=66.91 +/- 89.64\n",
      "Episode length: 21611.80 +/- 1539.62\n",
      "Eval num_timesteps=9900000, episode_reward=24.70 +/- 127.90\n",
      "Episode length: 22216.60 +/- 942.34\n",
      "Eval num_timesteps=10000000, episode_reward=-84.20 +/- 117.89\n",
      "Episode length: 22763.60 +/- 489.80\n"
     ]
    },
    {
     "data": {
      "text/plain": "<stable_baselines3.ppo.ppo.PPO at 0x1fb1b1f8f10>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.learn(total_timesteps=10000000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T07:21:56.569238Z",
     "start_time": "2024-02-27T07:21:48.227503700Z"
    }
   },
   "outputs": [],
   "source": [
    "# test episode\n",
    "obs, info = test_env.reset()\n",
    "balance_hist = []\n",
    "bench_hist = []\n",
    "position_hist = []\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    #test_env.render()\n",
    "\n",
    "    action, _ = rl_model.predict(obs)\n",
    "    obs, reward, done, trunc, info = test_env.step(action)\n",
    "\n",
    "    balance_hist.append(info['balance'])\n",
    "    bench_hist.append(info['midprice'])\n",
    "    position_hist.append(info['position'])\n",
    "\n",
    "test_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T07:21:59.422826400Z",
     "start_time": "2024-02-27T07:21:58.618005200Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input could not be cast to an at-least-1D NumPy array",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[0;32m      2\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m5\u001B[39m))\n\u001B[1;32m----> 3\u001B[0m \u001B[43mplt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mplot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbalance_hist\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m plt\u001B[38;5;241m.\u001B[39mshow()\n\u001B[0;32m      6\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m5\u001B[39m))\n",
      "File \u001B[1;32m~\\Dev\\deepcheese\\.venv\\Lib\\site-packages\\matplotlib\\pyplot.py:3575\u001B[0m, in \u001B[0;36mplot\u001B[1;34m(scalex, scaley, data, *args, **kwargs)\u001B[0m\n\u001B[0;32m   3567\u001B[0m \u001B[38;5;129m@_copy_docstring_and_deprecators\u001B[39m(Axes\u001B[38;5;241m.\u001B[39mplot)\n\u001B[0;32m   3568\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mplot\u001B[39m(\n\u001B[0;32m   3569\u001B[0m     \u001B[38;5;241m*\u001B[39margs: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m ArrayLike \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3573\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   3574\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m[Line2D]:\n\u001B[1;32m-> 3575\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgca\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mplot\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3576\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3577\u001B[0m \u001B[43m        \u001B[49m\u001B[43mscalex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscalex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3578\u001B[0m \u001B[43m        \u001B[49m\u001B[43mscaley\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscaley\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3579\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m}\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3580\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3581\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Dev\\deepcheese\\.venv\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:1721\u001B[0m, in \u001B[0;36mAxes.plot\u001B[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1478\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1479\u001B[0m \u001B[38;5;124;03mPlot y versus x as lines and/or markers.\u001B[39;00m\n\u001B[0;32m   1480\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1718\u001B[0m \u001B[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001B[39;00m\n\u001B[0;32m   1719\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1720\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m cbook\u001B[38;5;241m.\u001B[39mnormalize_kwargs(kwargs, mlines\u001B[38;5;241m.\u001B[39mLine2D)\n\u001B[1;32m-> 1721\u001B[0m lines \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_lines(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, data\u001B[38;5;241m=\u001B[39mdata, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)]\n\u001B[0;32m   1722\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m lines:\n\u001B[0;32m   1723\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_line(line)\n",
      "File \u001B[1;32m~\\Dev\\deepcheese\\.venv\\Lib\\site-packages\\matplotlib\\axes\\_base.py:303\u001B[0m, in \u001B[0;36m_process_plot_var_args.__call__\u001B[1;34m(self, axes, data, *args, **kwargs)\u001B[0m\n\u001B[0;32m    301\u001B[0m     this \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m args[\u001B[38;5;241m0\u001B[39m],\n\u001B[0;32m    302\u001B[0m     args \u001B[38;5;241m=\u001B[39m args[\u001B[38;5;241m1\u001B[39m:]\n\u001B[1;32m--> 303\u001B[0m \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_plot_args\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    304\u001B[0m \u001B[43m    \u001B[49m\u001B[43maxes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mthis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mambiguous_fmt_datakey\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mambiguous_fmt_datakey\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Dev\\deepcheese\\.venv\\Lib\\site-packages\\matplotlib\\axes\\_base.py:491\u001B[0m, in \u001B[0;36m_process_plot_var_args._plot_args\u001B[1;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001B[0m\n\u001B[0;32m    489\u001B[0m     y \u001B[38;5;241m=\u001B[39m _check_1d(xy[\u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m    490\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 491\u001B[0m     x, y \u001B[38;5;241m=\u001B[39m \u001B[43mindex_of\u001B[49m\u001B[43m(\u001B[49m\u001B[43mxy\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    493\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m axes\u001B[38;5;241m.\u001B[39mxaxis \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    494\u001B[0m     axes\u001B[38;5;241m.\u001B[39mxaxis\u001B[38;5;241m.\u001B[39mupdate_units(x)\n",
      "File \u001B[1;32m~\\Dev\\deepcheese\\.venv\\Lib\\site-packages\\matplotlib\\cbook.py:1672\u001B[0m, in \u001B[0;36mindex_of\u001B[1;34m(y)\u001B[0m\n\u001B[0;32m   1670\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1671\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39marange(y\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mfloat\u001B[39m), y\n\u001B[1;32m-> 1672\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mInput could not be cast to an at-least-1D NumPy array\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: Input could not be cast to an at-least-1D NumPy array"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x500 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(balance_hist)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(bench_hist)\n",
    "y = position_hist\n",
    "\n",
    "for i in range(len(y)):\n",
    "    if y[i] == 0:\n",
    "        pass\n",
    "    else:\n",
    "        if y[i] == 1:\n",
    "            color = 'red'\n",
    "        elif y[i] == -1:\n",
    "            color = 'blue'\n",
    "        plt.axvspan(i - 0.5, i + 0.5, color=color)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
